# Blue-Green (Canary) деплоймент

На прошлом этапе мы настроили blue-green балансировку. На этом ею воспользуемся.

## Выложим в Blue версию с ошибкой

Чтобы увидеть, как работает blue-green деплой в случае ошибок в новой версии приложения, попробуем выложить проблемную
версию приложения в blue и перенаправить на него часть трафика.

Генерировать пользовательскую нагрузку будем скриптом:
```bash
cat ./steps/8_blue_green_deploy/load_clock_service.sh
```

Откроем новое, **отдельное** окно терминала, и запустим там его:
```bash
./steps/8_blue_green_deploy/load_clock_service.sh
```

Вернёмся в основное окно терминала.

Запустим команду, которая внедрит в приложение ошибку возникающую с некоторой вероятностью:
```bash
./steps/8_blue_green_deploy/make_clock_app_broken.sh
```

Соберём её и выложим в реестр контейнеров:
```bash
./apps/clock/build_and_push_conatiner_image.sh broken
```

Переопределим образ в blue кастомизации и поднимем число реплик:
```bash
REGISTRY=$(yc container registry get workshop --format json | jq .id -r | tee /dev/stderr)
# Тег образа переопределяется на 'broken' - который был собран выше
cat <<EOF >> deploy/clock/blue/kustomization.yaml
images:
- name: cr.yandex/${REGISTRY:?}/clock
  newTag: broken
EOF

# Число реплик 0 -> 3
perl -pe 's/0/3/g' -i deploy/clock/blue/set_replicas.yaml
```

Посмотрим что поменяется:
```bash
kubectl diff -k deploy/clock
```

Смело применим, т.к. трафик на blue деплоймент пока не подаётся.
```bash
kubectl apply -k deploy/clock
kubectl rollout status deployment/clock-blue
```

Поменяем веса балансировки в настройках группы бекендов сервиса `clock`. На green оставим 80%, а на blue направим 20%:
```bash
# Скопируем текущую конфигурацию, на случай если нужно будет быстро вернуть
\cp ./deploy/clock_backend_group.yaml ./clock_backends_green_only.yaml

# Поменяем веса 0 -> 20; 100 -> 80
perl -pe 's/weight: 0/weight: 20/g; s/weight: 100/weight: 80/g' -i ./deploy/clock_backend_group.yaml

# Убедимся, что изменения ожидаемые
kubectl diff -f deploy
```
В реальности стоит начинать с 5%, или даже меньше.

Применим:
```bash
kubectl apply -f deploy
```

Снова откроем карту балансировки в Web UI:
```bash
./steps/6_publish_apps_using_application_load_balancer/alb_web_ui_link.sh
```

Убедимся, что веса балансировки поменялись, и стали 80 и 20.

Не закрывая вкладку вернёмся в терминал, откроем окно с генератором нагрузки. Увидим, что начали появляться
ответы `FAIL 500`.

Вернёмся в Web UI. Теперь откроем _в отдельной вкладке_ (`Ctrl(Cmd на macOS)+Клик`) раздел _Мониторинг_ слева. Выберем
масштаб _1h_, увеличим ещё пару раз нажав _+_ и нажав кнопку _Сейчас_ справа.

В панели _5XX_ справа вверху график пойдёт вверх, указывая на то, что есть ответы с HTTP кодами
серверных ошибок.

Убедимся что ошибки на blue бекенде. Откроем _в отдельной вкладке_ дашборд в сервисе Yandex Monitoring кнопкой _Открыть
в Мониторинге_. Вернёмся во вкладку с картой балансировки, и запомним префикс ID бекенда с весом 20. Перейдём на
вкладку мониторинга, и для переменной _Backend_ выберем этот бекенд. Снова выберем масштаб _1h_, увеличим ещё пару раз
нажав _+_ и нажав кнопку _Сейчас_ справа. Увидим что идут ошибки, с уровнем эквивалентным суммарному. Вернёмся на
вкладку с картой балансировки, запомним префикс ID бекенда с весом 80. Вернёмся на вкладку с дашбордом Yandex
Monitoring, выберем этот бекенд. Увидим что ошибок нет. Из чего вывод, что новая версия приложения проблемная, и её
нужно откатить.

## Откатим проблемную версию

Вернём исходную версию конфигурации обратно:
```bash
mv -f ./clock_backends_green_only.yaml ./deploy/clock_backend_group.yaml 
# Убедимся, что изменения ожидаемые
kubectl diff -f deploy
```

Применим:
```bash
kubectl apply -f deploy
```

Вернёмся на вкладку с открытой картой балансировки, обновим её. Увидим, что веса стали 100 и 0. Откроем вкладку с
разделом _Мониторинг_ сервиса ALB, увидим что ошибки сошли в ноль.

Трафик с blue деплоймента ушёл, значит его можно свернуть, чтобы освободить  ресурсы в кластере.
Вернём число реплик в blue деплойменте в 0:
```bash
# Число реплик 3 -> 0
perl -pe 's/3/0/g' -i deploy/clock/blue/set_replicas.yaml
# Убедимся, что изменения ожидаемые
kubectl diff -k deploy/clock
```

Применим:
```bash
kubectl apply -k deploy/clock
```

Откроем окно терминала с генератором нагрузки, увидим что идут только `ok`. Остановим его нажав `Ctrl+C`


Итого, мы:
* разделили приложение на blue-green
* выкатили на blue новую версию
* направили на неё часть трафика
* увидили проблемы
* вернули трафик обратно
* свернули blue 

В реальности, дальше нужно идти чинить приложение и пробовать снова.







